mpicc -c write_vtk.c
mpifort -c -O3 -ftree-vectorize -march=native -ffast-math -funroll-loops -fopenmp main.f90
mpifort -O3 -ftree-vectorize -march=native -ffast-math -funroll-loops -fopenmp main.o write_vtk.o -o runme
mpirun -np 8 --map-by dist:span --mca rmaps_dist_device ib0 --report-bindings ./runme 8 8 2 2 2 0 2>&1 > HPCdata/N8_P8_px2_py2_pz2_0

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 9603230: <profiling> in cluster <dcc> Done

Job <profiling> was submitted from host <n-62-30-8> by user <s164331> in cluster <dcc> at Fri Apr 30 20:01:20 2021
Job was executed on host(s) <24*n-62-21-75>, in queue <hpcintro>, as user <s164331> in cluster <dcc> at Fri Apr 30 20:01:22 2021
                            <24*n-62-21-74>
                            <24*n-62-21-76>
</zhome/a0/5/117468> was used as the home directory.
</zhome/a0/5/117468/Kurser/LSM/Project2/RB Gauss Seidel, Fortran> was used as the working directory.
Started at Fri Apr 30 20:01:22 2021
Terminated at Fri Apr 30 20:01:28 2021
Results reported at Fri Apr 30 20:01:28 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -n 72
#BSUB -R "span[block=24]"
#BSUB -W 00:59
#BSUB -q hpcintro
#BSUB -R "rusage[mem=3GB]"
#BSUB -J profiling
#BSUB -o HPCOutputs/Output_%J.out
#BSUB -e HPCOutputs/Error_%J.err

module purge
module load mpi/3.1.3-gcc-8.2.0

# do a clean build to ensure everything is updated
make clean
make

# Loop matrix size
for N in 8 #27 64 125 216 343 512 729 1000 #1331 1728 2197
do

# Loop px
for all_p in 2 3 4
do
px=$((all_p))
py=$((all_p))
pz=$((all_p))

np=$((px*py*pz))

# skip cases where np > N
[ $np -gt $N ] && continue

# skip calculations with more processors than available
[ $np -gt $LSB_DJOB_NUMPROC ] && continue

# adjust here for the different algorithms
for algo in 0
do
out="HPCdata/N${N}_P${np}_px${px}_py${py}_pz${pz}_${algo}"
[ -e $out ] && continue

# Adjust arguments to your project
echo "mpirun -np $np --map-by dist:span --mca rmaps_dist_device ib0 \
--report-bindings \
./runme $N $np $px $py $pz $algo 2>&1 > $out"

mpirun -np $np --map-by dist:span --mca rmaps_dist_device ib0 \
       --report-bindings \
       ./runme $N $np $px $py $pz $algo > $out 2>${out}_binding

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3.10 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     221184.00 MB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   6 sec.
    Turnaround time :                            8 sec.

The output (if any) is above this job summary.



PS:

Read file <HPCOutputs/Error_9603230.err> for stderr output of this job.

